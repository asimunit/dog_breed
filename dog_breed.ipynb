{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.0\n",
      "Torchvision Version:  0.8.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "import time\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import shutil\n",
    "\n",
    "source_files = '/home/asim/Desktop/dog_breed/train/'\n",
    "train_destination_folder = '/home/asim/Desktop/dog_breed/data/train/'\n",
    "val_destination_folder = '/home/asim/Desktop/dog_breed/data/val/'\n",
    "try:\n",
    "    shutil.rmtree(train_destination_folder)\n",
    "    shutil.rmtree(val_destination_folder)\n",
    "except:\n",
    "    print(\"weew\")\n",
    "os.mkdir(train_destination_folder)\n",
    "os.mkdir(val_destination_folder)\n",
    "with open(\"labels.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    index = 0\n",
    "    for i, line in enumerate(reader):\n",
    "        \n",
    "        temp = line[0].split(',')\n",
    "        name = temp[0]\n",
    "        breed = temp[1]\n",
    "#         print(name,breed)\n",
    "        \n",
    "        if name != \"id\":\n",
    "            directory  = os.path.isdir(train_destination_folder + breed)\n",
    "            if directory ==True:\n",
    "                no_of_files = len(os.listdir(train_destination_folder+breed)) \n",
    "#                 print(no_of_files)\n",
    "              \n",
    "                if index%5 != 0:\n",
    "                    \n",
    "                    shutil.copy(source_files + name + \".jpg\", train_destination_folder + breed )\n",
    "                    index +=1\n",
    "                else:\n",
    "                    \n",
    "                    val_directory  = os.path.isdir(val_destination_folder + breed)\n",
    "                    if val_directory == True:\n",
    "                        shutil.copy(source_files + name + \".jpg\", val_destination_folder + breed )\n",
    "                        index += 1\n",
    "                    else:\n",
    "                        os.mkdir(val_destination_folder + breed)\n",
    "                        shutil.copy(source_files + name + \".jpg\", val_destination_folder + breed)\n",
    "                        index += 1\n",
    "            else:\n",
    "                os.mkdir(train_destination_folder + breed)\n",
    "                shutil.copy(source_files + name + \".jpg\", train_destination_folder + breed)\n",
    "                index +=1 \n",
    "              \n",
    "       \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\"\n",
    "\n",
    "\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = len(os.listdir(train_destination_folder))\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 50\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "        if feature_extracting:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
      ")\n",
      "Initializing Datasets and Dataloaders...\n",
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.5823 Acc: 0.2623\n",
      "val Loss: 1.5340 Acc: 0.6677\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.1873 Acc: 0.5402\n",
      "val Loss: 0.9109 Acc: 0.7791\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.7666 Acc: 0.6044\n",
      "val Loss: 0.7040 Acc: 0.8103\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.5694 Acc: 0.6266\n",
      "val Loss: 0.6144 Acc: 0.8212\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.4526 Acc: 0.6421\n",
      "val Loss: 0.5514 Acc: 0.8291\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.3925 Acc: 0.6555\n",
      "val Loss: 0.5458 Acc: 0.8266\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.3405 Acc: 0.6604\n",
      "val Loss: 0.5188 Acc: 0.8326\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.2802 Acc: 0.6705\n",
      "val Loss: 0.4890 Acc: 0.8450\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.2763 Acc: 0.6676\n",
      "val Loss: 0.4833 Acc: 0.8375\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.2470 Acc: 0.6762\n",
      "val Loss: 0.4941 Acc: 0.8455\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.2197 Acc: 0.6782\n",
      "val Loss: 0.4856 Acc: 0.8479\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.2089 Acc: 0.6811\n",
      "val Loss: 0.4864 Acc: 0.8465\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.1818 Acc: 0.6895\n",
      "val Loss: 0.4962 Acc: 0.8400\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.1974 Acc: 0.6833\n",
      "val Loss: 0.4984 Acc: 0.8326\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.1476 Acc: 0.6982\n",
      "val Loss: 0.4940 Acc: 0.8474\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.1521 Acc: 0.6901\n",
      "val Loss: 0.4933 Acc: 0.8445\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.1695 Acc: 0.6895\n",
      "val Loss: 0.4681 Acc: 0.8479\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.1167 Acc: 0.6986\n",
      "val Loss: 0.4756 Acc: 0.8450\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.1104 Acc: 0.7007\n",
      "val Loss: 0.4796 Acc: 0.8450\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.1349 Acc: 0.6928\n",
      "val Loss: 0.4932 Acc: 0.8405\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.1425 Acc: 0.6996\n",
      "val Loss: 0.4748 Acc: 0.8470\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.0857 Acc: 0.7036\n",
      "val Loss: 0.4996 Acc: 0.8494\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.0701 Acc: 0.7099\n",
      "val Loss: 0.5120 Acc: 0.8435\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.0819 Acc: 0.7064\n",
      "val Loss: 0.4837 Acc: 0.8474\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.0580 Acc: 0.7136\n",
      "val Loss: 0.4998 Acc: 0.8465\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.1212 Acc: 0.6962\n",
      "val Loss: 0.4992 Acc: 0.8470\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.0830 Acc: 0.7134\n",
      "val Loss: 0.5122 Acc: 0.8400\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.0737 Acc: 0.7082\n",
      "val Loss: 0.4670 Acc: 0.8479\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.0650 Acc: 0.7103\n",
      "val Loss: 0.4808 Acc: 0.8474\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.0410 Acc: 0.7218\n",
      "val Loss: 0.4753 Acc: 0.8484\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.0574 Acc: 0.7195\n",
      "val Loss: 0.4889 Acc: 0.8479\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.0591 Acc: 0.7132\n",
      "val Loss: 0.4774 Acc: 0.8559\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.0578 Acc: 0.7127\n",
      "val Loss: 0.4743 Acc: 0.8519\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.0649 Acc: 0.7144\n",
      "val Loss: 0.4892 Acc: 0.8470\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.0519 Acc: 0.7172\n",
      "val Loss: 0.5007 Acc: 0.8465\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.0384 Acc: 0.7169\n",
      "val Loss: 0.4799 Acc: 0.8470\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.0179 Acc: 0.7217\n",
      "val Loss: 0.5028 Acc: 0.8395\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.0345 Acc: 0.7182\n",
      "val Loss: 0.4992 Acc: 0.8489\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.0248 Acc: 0.7212\n",
      "val Loss: 0.4955 Acc: 0.8504\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.0289 Acc: 0.7238\n",
      "val Loss: 0.4881 Acc: 0.8440\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.0456 Acc: 0.7196\n",
      "val Loss: 0.4754 Acc: 0.8519\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.0287 Acc: 0.7162\n",
      "val Loss: 0.4775 Acc: 0.8524\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.0259 Acc: 0.7144\n",
      "val Loss: 0.4844 Acc: 0.8465\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.0230 Acc: 0.7235\n",
      "val Loss: 0.5028 Acc: 0.8405\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.0354 Acc: 0.7218\n",
      "val Loss: 0.4882 Acc: 0.8534\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.0131 Acc: 0.7289\n",
      "val Loss: 0.5221 Acc: 0.8420\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.0098 Acc: 0.7246\n",
      "val Loss: 0.4983 Acc: 0.8445\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.0127 Acc: 0.7213\n",
      "val Loss: 0.4835 Acc: 0.8499\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.9941 Acc: 0.7306\n",
      "val Loss: 0.5026 Acc: 0.8489\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.9852 Acc: 0.7314\n",
      "val Loss: 0.5235 Acc: 0.8385\n",
      "\n",
      "Training complete in 982m 20s\n",
      "Best val Acc: 0.855869\n",
      "59011.08942055702\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-7-eef953771c56>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-eef953771c56>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    return \"Model Trained\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "\n",
    "# %%\n",
    "torch.save(model_ft, 'inception_floorplan.pth')\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
